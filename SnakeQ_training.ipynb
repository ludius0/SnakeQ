{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SnakeQ training.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MB0lKXKXTJF1"
      },
      "source": [
        "**AUTHORIZATIONS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urMGZWc_S6Uh"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('./mount')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXulFYZFTLGm"
      },
      "source": [
        "**IMPORT LIBRARIES**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKVNYPo7TPPb"
      },
      "source": [
        "import collections\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nW6NLtK0TfAg"
      },
      "source": [
        "**Parameters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32M_uGnOTfJw"
      },
      "source": [
        "# Board\n",
        "row = 10\n",
        "\n",
        "# Neural Network and Q-learning\n",
        "GAMMA = 0.99  # discount\n",
        "BATCH_SIZE = 64\n",
        "REPLAY_SIZE = 10000\n",
        "LEARNING_RATE = 1e-4\n",
        "SYNC_TARGET_LOOPS = 1000\n",
        "REPLAY_START_SIZE = 10000\n",
        "\n",
        "# epsilon -> chance for random action\n",
        "EPSILON_DECAY_LAST_FRAME = 10**5\n",
        "EPSILON_START = 1.0\n",
        "EPSILON_FINAL = 0.02\n",
        "\n",
        "reward_dict = {\"hit self\": -100, \"hit boundary\": -100, \"eat apple\": 30, \"step\": -1, \"see apple\": 1, \"a lot of steps\": -10, \"win game\": 1000}"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aakFnMylTpdb"
      },
      "source": [
        "**Environment (Snake game)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzdmGDMqTpkf"
      },
      "source": [
        "class SnakeSensors:\n",
        "    def __init__(self, x, board_info, moves):\n",
        "        \"\"\"\n",
        "        Return array where are saved all information about snake and his sensors,\n",
        "        which will get pass to agent through neural network for choosing action\n",
        "        Sensors:\n",
        "            1. distance to walls (4 dim)\n",
        "            2. distance to apple (8 dim)\n",
        "            3. distance to body snake (8 dim)\n",
        "            4. snake head direction\n",
        "            5. snake tail direction\n",
        "            Note: distance is converted between 0.0 to 1.0\n",
        "        \"\"\"\n",
        "        self.dis = x    # row/x/distance\n",
        "        self.target = board_info\n",
        "        self.moves = moves\n",
        "    \n",
        "    def update_sensor_board(self, board, apple, snake):\n",
        "        self.board = board\n",
        "        self.apple_pos = apple\n",
        "        self.head_y, self.head_x = snake[-1]\n",
        "\n",
        "        # distance to boundaries in 4 dimensions\n",
        "        self.dis_y_down = self.dis - self.head_y - 1    # -1 because grid is from 0 to boundary\n",
        "        self.dis_y_up = self.dis - self.dis_y_down - 1\n",
        "        self.dis_x_right = self.dis - self.head_x - 1\n",
        "        self.dis_x_left = self.dis - self.dis_x_right - 1\n",
        "    \n",
        "    def check_up(self, target):\n",
        "        if self.head_y == 0: return 0\n",
        "        for i in range(self.head_y-1, -1, -1):\n",
        "            if self.board[i, self.head_x] == target:\n",
        "                return 1\n",
        "        return 0\n",
        "    \n",
        "    def check_down(self, target):\n",
        "        if self.head_y == self.dis-1: return 0\n",
        "        for i in range(self.head_y+1, self.dis):\n",
        "            if self.board[i, self.head_x] == target:\n",
        "                return 1\n",
        "        return 0\n",
        "    \n",
        "    def check_right(self, target):\n",
        "        if self.head_x == self.dis-1: return 0\n",
        "        for i in range(self.head_x+1, self.dis):\n",
        "            if self.board[self.head_y, i] == target:\n",
        "                return 1\n",
        "        return 0\n",
        "    \n",
        "    def check_left(self, target):\n",
        "        if self.head_x == 0: return 0\n",
        "        for i in range(self.head_x-1, -1, -1):\n",
        "            if self.board[self.head_y, i] == target:\n",
        "                return 1\n",
        "        return 0\n",
        "    \n",
        "    def check_right_up(self, target):\n",
        "        # choose shorter distance\n",
        "        distance = self.dis_y_up if self.dis_y_up < self.dis_x_right else self.dis_x_right\n",
        "        if distance == 0: return 0\n",
        "        for n in range(1, distance+1):\n",
        "            if self.board[self.head_y-n, self.head_x+n] == target:\n",
        "                return 1\n",
        "        return 0\n",
        "    \n",
        "    def check_right_down(self, target):\n",
        "        distance = self.dis_y_down if self.dis_y_down < self.dis_x_right else self.dis_x_right\n",
        "        if distance == self.dis: return 0\n",
        "        for n in range(1, distance+1):\n",
        "            if self.board[self.head_y+n, self.head_x+n] == target: \n",
        "                return 1\n",
        "        return 0\n",
        "    \n",
        "    def check_left_up(self, target):\n",
        "        distance = self.dis_y_up if self.dis_y_up < self.dis_x_left else self.dis_x_left\n",
        "        if distance == 0: return 0\n",
        "        for n in range(1, distance+1):\n",
        "            if self.board[self.head_y-n, self.head_x-n] == target: \n",
        "                return 1\n",
        "        return 0\n",
        "\n",
        "    def check_left_down(self, target):\n",
        "        distance = self.dis_y_down if self.dis_y_down < self.dis_x_left else self.dis_x_left\n",
        "        if distance == self.dis: return 0\n",
        "        for n in range(1, distance+1):\n",
        "            if self.board[self.head_y+n, self.head_x-n] == target: \n",
        "                return 1\n",
        "        return 0\n",
        "    \n",
        "    def all_eight_directions(self, target):\n",
        "        to_up = self.check_up(target)                       # up\n",
        "        to_right_up = self.check_right_up(target)           # right up\n",
        "        to_right = self.check_right(target)                 # right\n",
        "        to_right_down = self.check_right_down(target)       # right down\n",
        "        to_down = self.check_down(target)                   # down\n",
        "        to_left_down = self.check_left_down(target)         # left down\n",
        "        to_left = self.check_left(target)                   # left\n",
        "        to_left_up = self.check_left_up(target)             # left up\n",
        "        return np.array([to_up, to_right_up, to_right, to_right_down, to_down, to_left_down, to_left, to_left_up])\n",
        "    \n",
        "    def distance_to_wall(self):\n",
        "        return np.round(np.array([self.dis_y_up, self.dis_x_right, self.dis_y_down, self.dis_x_left]) / self.dis, 1)\n",
        "\n",
        "    def see_apple(self):\n",
        "        return self.all_eight_directions(self.target[\"apple\"])\n",
        "\n",
        "    def see_it_self(self):\n",
        "        return self.all_eight_directions(self.target[\"snake\"]) # skip snake head\n",
        "    \n",
        "    def get_head_direction(self, head_dir):\n",
        "        if np.array_equal(head_dir, self.moves[\"up\"]): return np.array([1, 0, 0, 0])   # up\n",
        "        elif np.array_equal(head_dir, self.moves[\"right\"]): return np.array([0, 1, 0, 0])  # right\n",
        "        elif np.array_equal(head_dir, self.moves[\"down\"]): return np.array([0, 0, 1, 0])  # down\n",
        "        elif np.array_equal(head_dir, self.moves[\"left\"]): return np.array([0, 0, 0, 1]) # left\n",
        "    \n",
        "    def get_tail_direction(self, tail_dir):\n",
        "        if np.array_equal(tail_dir, self.moves[\"up\"]): return np.array([1, 0, 0, 0])  # up\n",
        "        elif np.array_equal(tail_dir, self.moves[\"right\"]): return np.array([0, 1, 0, 0]) # right\n",
        "        elif np.array_equal(tail_dir, self.moves[\"down\"]): return np.array([0, 0, 1, 0]) # down\n",
        "        elif np.array_equal(tail_dir, self.moves[\"left\"]): return np.array([0, 0, 0, 1])# left\n",
        "\n",
        "\n",
        "###############################################################################################\n",
        "class Environment():\n",
        "    def __init__(self, n_row):\n",
        "        # parameters\n",
        "        self.board_info = {\"empty\": 0, \"snake\": 1, \"apple\": 2}\n",
        "        # (y, x)\n",
        "        self.moves = {\"up\": np.array([-1, 0]), \"right\": np.array([0, 1]), \"down\": np.array([1, 0]), \"left\": np.array([0, -1])}\n",
        "        self.reward_dict = reward_dict\n",
        "        self.count_deaths = -2\n",
        "        # Snake sensors for returning state (all logic)\n",
        "        self.Sensors = SnakeSensors(n_row, self.board_info, self.moves)\n",
        "\n",
        "        # Generate components\n",
        "        self.x = n_row\n",
        "        self.reward = 0   # reward based on every action and its consequences\n",
        "        self.restart_env()\n",
        "    \n",
        "    def generate_grid(self):\n",
        "        # generate board (grid) of zeros (always square)\n",
        "        self.board = np.zeros((self.x, self.x))\n",
        "    \n",
        "    def generate_snake(self):\n",
        "        # Randomly choose spot to generate snake\n",
        "        indices = np.random.randint(2, high=self.x-2, size=2)\n",
        "        y, x = indices[0], indices[1]\n",
        "\n",
        "        self.board[y, x] = 1\n",
        "        self.snake_body = np.array([[y, x-2], [y, x-1], [y, x]])\n",
        "\n",
        "\n",
        "    def generate_apple(self):\n",
        "        # Randomly generate apple (if there isn't already body of snake)\n",
        "        while True:\n",
        "            indices = np.random.randint(0, high=self.x, size=2)\n",
        "            y, x = indices[0], indices[1]\n",
        "\n",
        "            if self.board[y, x] == 0:\n",
        "                self.board[y, x] = 2\n",
        "                self.apple_pos = np.array([y, x])\n",
        "                break\n",
        "        \n",
        "    def collision_with_self(self):\n",
        "        # if count of eaten apples don't equel size of snake body -> collision with it self; game over\n",
        "        body = []\n",
        "        snake_body = self.snake_body.tolist()\n",
        "        for i in snake_body:\n",
        "            body.append((i[0], i[1]))\n",
        "        \n",
        "        if self.eaten_apples+1 != len(set(body)):\n",
        "            self.done = True\n",
        "            self.reward = self.reward_dict[\"hit self\"]\n",
        "    \n",
        "    def collision_with_boundaries(self):\n",
        "        # if snake go beyond board; game over\n",
        "        if (self.snake_body[-1, 0] < 0 or self.snake_body[-1, 0] >= self.x) \\\n",
        "            or (self.snake_body[-1, 1] < 0 or self.snake_body[-1, 1] >= self.x):\n",
        "            self.done = True\n",
        "            self.reward = self.reward_dict[\"hit boundary\"]\n",
        "\n",
        "    def collision_with_apple(self):\n",
        "        # Add another body to snake and generate another apple\n",
        "        if np.array_equal(self.snake_body[-1], self.apple_pos):\n",
        "            self.eaten_apples += 1\n",
        "            self.reward = self.reward_dict[\"eat apple\"]\n",
        "            self.steps = 0\n",
        "\n",
        "            self.check_for_end()\n",
        "            if not self.done: \n",
        "                self.generate_apple()\n",
        "        else:\n",
        "            self.pop_snake_tail()\n",
        "        self.get_tail_dir()\n",
        "\n",
        "    def check_steps(self):\n",
        "        # If taken (row*row) or more steps until apple is eaten -> game over\n",
        "        if self.steps > (self.x*self.x//2):\n",
        "            self.done = True\n",
        "            self.reward = self.reward_dict[\"a lot of steps\"]\n",
        "    \n",
        "    def check_for_end(self):\n",
        "        # return True if snake filled whole board else False\n",
        "        if np.all(self.board.all(1)):\n",
        "            self.done = True\n",
        "            self.game_info = {\"won game\": True}\n",
        "    \n",
        "    def restart_env(self):\n",
        "        # set up parameters\n",
        "        self.apple_pos = None           # position of current apple\n",
        "        self.none = None\n",
        "        self.head_dir = self.none       # direction of head of snake\n",
        "        self.prev_direction = None\n",
        "        self.steps = 0                  # every action untill apple is eaten\n",
        "        self.done = False               # if env/game is finished\n",
        "        self.game_info = {\"won game\": False} # info about state of game\n",
        "        self.count_deaths += 1          # Count generations\n",
        "\n",
        "        # Generate game grid\n",
        "        self.generate_grid()\n",
        "        self.generate_snake()\n",
        "        self.generate_apple()\n",
        "\n",
        "        self.eaten_apples = len(self.snake_body)           # track of eaten apple\n",
        "        self.get_tail_dir()\n",
        "\n",
        "        self.check_state()\n",
        "\n",
        "        return self.state\n",
        "\n",
        "    def update_board(self):\n",
        "        # refresh board; write on board snake and apple\n",
        "        self.generate_grid()\n",
        "        for body in self.snake_body:\n",
        "            self.board[body[0], body[1]] = 1\n",
        "        \n",
        "        self.board[self.apple_pos[0], self.apple_pos[1]] = 2\n",
        "\n",
        "    def check_state(self):\n",
        "        self.Sensors.update_sensor_board(self.board, self.apple_pos, self.snake_body)\n",
        "        distance_sensor = self.Sensors.distance_to_wall()\n",
        "        apple_sensor = self.Sensors.see_apple()\n",
        "        snake_body_sensor = self.Sensors.see_it_self()\n",
        "        if self.head_dir is not self.none: head_dir_sensor = self.Sensors.get_head_direction(self.head_dir)\n",
        "        else: head_dir_sensor = np.array([0, 0, 0, 0])\n",
        "\n",
        "        if len(self.snake_body) > 1: tail_dir_sensor = self.Sensors.get_tail_direction(self.tail_dir)\n",
        "        else: tail_dir_sensor = np.array([0, 0, 0, 0])\n",
        "\n",
        "        self.state = np.concatenate((distance_sensor, apple_sensor, snake_body_sensor, head_dir_sensor, tail_dir_sensor), axis=0)\n",
        "    \n",
        "    def select_random_action(self):\n",
        "        list_of_action = np.array([0, 1, 2, 3])\n",
        "        return np.random.choice(list_of_action, size=1)[0]\n",
        "    \n",
        "    def get_head_dir(self, direction):\n",
        "        # snake cannot kill himself by going opposite direction\n",
        "        if direction == 0 and self.prev_direction != 2:         self.head_dir = self.moves[\"up\"]\n",
        "        elif direction == 1 and self.prev_direction != 3:       self.head_dir = self.moves[\"right\"]\n",
        "        elif direction == 2 and self.prev_direction != 0:       self.head_dir = self.moves[\"down\"]\n",
        "        elif direction == 3 and self.prev_direction != 1:       self.head_dir = self.moves[\"left\"]\n",
        "\n",
        "        self.prev_direction = direction\n",
        "    \n",
        "    def get_tail_dir(self):\n",
        "        if len(self.snake_body) > 1:\n",
        "            self.tail_dir = tuple((np.array([self.snake_body[1, 0], self.snake_body[1, 1]] \\\n",
        "                 - np.array([self.snake_body[0, 0], self.snake_body[0, 1]]))).reshape(1, -1)[0])\n",
        "    \n",
        "    def reward_for_steps(self):\n",
        "        # if snake see apple, than get reward otherwise is punished\n",
        "        if 1 in self.state[4:12]:\n",
        "            self.reward = self.reward_dict[\"see apple\"]\n",
        "        else:\n",
        "            self.reward = self.reward_dict[\"step\"]\n",
        "    \n",
        "    def pop_snake_tail(self):\n",
        "        if self.eaten_apples == len(self.snake_body)-1 and not self.done:\n",
        "            self.snake_body = np.delete(self.snake_body, 0, 0)\n",
        "            self.reward_for_steps()\n",
        "\n",
        "    def snake_move(self, direction):\n",
        "        \"\"\"\n",
        "        Algorithm: add new part before head in corresponding direction and delete tail\n",
        "        \"\"\"\n",
        "        self.get_head_dir(direction)\n",
        "        head = self.snake_body[-1]\n",
        "        new_head = np.array([head[0]+self.head_dir[0], head[1]+self.head_dir[1]])\n",
        "        self.snake_body = np.vstack((self.snake_body, new_head))\n",
        "        # deletng tail is handled in collision_with_apple() with getting tail direction and reward for nothing happening\n",
        "\n",
        "        self.steps += 1\n",
        "    \n",
        "    def action(self, act):\n",
        "        # Set up\n",
        "        self.reward = 0\n",
        "        if self.done: \n",
        "            self.restart_env()\n",
        "\n",
        "        # Check game logic\n",
        "        self.snake_move(act)\n",
        "        self.collision_with_self()\n",
        "        self.collision_with_apple()\n",
        "        self.collision_with_boundaries()\n",
        "        self.check_steps()\n",
        "        self.check_for_end()\n",
        "\n",
        "        # Update\n",
        "        if not self.done: \n",
        "            self.update_board()\n",
        "            self.check_state()\n",
        "\n",
        "        return self.state, self.reward, self.done, self.game_info"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxCuAG-1UADt"
      },
      "source": [
        "**DQN AI**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDYWb6UDUAOA"
      },
      "source": [
        "class Neural_Network(nn.Module):\n",
        "    def __init__(self, input_size=28, lr=LEARNING_RATE):\n",
        "        super().__init__()\n",
        "        \"\"\"\n",
        "        Input to NN:\n",
        "            [distance to wall, see apple, see it self, head direction, tail direction] -> 28 elements\n",
        "        output of NN:\n",
        "            [0: up    1: right    2: down    3: left] -> 4 elements\n",
        "        \"\"\"\n",
        "        # Neural Network\n",
        "        \n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_size, 20),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(20, 12),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(12, 4),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        self.loss_f = nn.BCELoss()\n",
        "        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
        "    \n",
        "    def forward(self, input_tensor):\n",
        "        return self.model(input_tensor)\n",
        "\n",
        "class ExperienceBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = collections.deque(maxlen=capacity)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "    def append(self, experience):\n",
        "        self.buffer.append(experience)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
        "        states, actions, rewards, dones, next_states = zip(*[self.buffer[idx] for idx in indices])\n",
        "        return np.array(states), np.array(actions), np.array(rewards, dtype=np.float32), \\\n",
        "               np.array(dones, dtype=np.uint8), np.array(next_states)\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, env, buffer):\n",
        "        self.env = env\n",
        "        self.buffer = buffer\n",
        "        self.Experience = collections.namedtuple('Experience', field_names=['state', 'action', 'reward', 'done', 'new_state'])\n",
        "        self._reset(env)\n",
        "\n",
        "    def _reset(self, env):\n",
        "        self.state = env.restart_env()\n",
        "        self.total_reward_ = 0.0\n",
        "\n",
        "    def play_step(self, net, env, epsilon=0.0, device=\"cpu\"):\n",
        "        done_reward = None\n",
        "\n",
        "        if np.random.random() < epsilon:\n",
        "            act = env.select_random_action()\n",
        "        else:\n",
        "            state_a = np.array([self.state], copy=False)\n",
        "            state_v = torch.from_numpy(state_a).to(device, dtype=torch.float32)\n",
        "            q_vals_v = net.forward(state_v)\n",
        "            _, act_v = torch.max(q_vals_v, dim=1)\n",
        "            act = int(act_v.item())\n",
        "\n",
        "        # do step in the environment\n",
        "        new_state, reward, is_done, _ = env.action(act)\n",
        "        self.total_reward_ += reward\n",
        "\n",
        "        exp = self.Experience(self.state, act, reward, is_done, new_state)\n",
        "        self.buffer.append(exp)\n",
        "        self.state = new_state\n",
        "\n",
        "        if is_done:\n",
        "            done_reward = self.total_reward_\n",
        "            self._reset(env)\n",
        "        return done_reward\n",
        "\n",
        "class DQN(Agent):\n",
        "    def __init__(self, env, buffer, net, load=False):\n",
        "        super().__init__(env, buffer)\n",
        "        self.device = self.select_device()\n",
        "        # Networks\n",
        "        self.net = Neural_Network().to(self.device)\n",
        "        self.target_net = Neural_Network().to(self.device)\n",
        "\n",
        "        # parameters\n",
        "        self.total_rewards = []\n",
        "        self.best_mean_reward = None\n",
        "        self.mean_reward = None\n",
        "        self.index = 0\n",
        "        self.epsilon = EPSILON_START\n",
        "        self.episode = 0\n",
        "\n",
        "        self.agent_info = {}\n",
        "\n",
        "        # loading sequence\n",
        "        if load: self.load()\n",
        "\n",
        "    def select_device(self):\n",
        "        if torch.cuda.is_available():\n",
        "            torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
        "            print(\"using cuda:\", torch.cuda.get_device_name(0))\n",
        "        return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    def load(self, load_index=True, load_episode=True):\n",
        "        self.net.load_state_dict(torch.load(\"mount/My Drive/Colab Notebooks/snake/net.dat\", map_location=torch.device(self.device)))\n",
        "        self.target_net.load_state_dict(self.net.state_dict())\n",
        "        if not load_index:\n",
        "          with open(\"mount/My Drive/Colab Notebooks/snake/index\", 'rb') as f:\n",
        "            self.index = np.load(f)[0]\n",
        "        with open(\"mount/My Drive/Colab Notebooks/snake/total_rewards\", 'rb') as f:\n",
        "          self.total_rewards = np.load(f).tolist()\n",
        "        with open(\"mount/My Drive/Colab Notebooks/snake/count_deaths\", 'rb') as f:\n",
        "          self.env.count_deaths = np.load(f)[0]\n",
        "        if not load_episode:\n",
        "          with open(\"mount/My Drive/Colab Notebooks/snake/episode\", 'rb') as f:\n",
        "              self.episode = np.load(f)[0]\n",
        "    \n",
        "    def save(self):\n",
        "        torch.save(self.net.state_dict(), \"mount/My Drive/Colab Notebooks/snake/net.dat\")\n",
        "        with open(\"mount/My Drive/Colab Notebooks/snake/index\", 'wb') as f:\n",
        "          np.save(f, np.array([self.index]))\n",
        "        with open(\"mount/My Drive/Colab Notebooks/snake/total_rewards\", 'wb') as f:\n",
        "          np.save(f, self.total_rewards)\n",
        "        with open(\"mount/My Drive/Colab Notebooks/snake/count_deaths\", 'wb') as f:\n",
        "          np.save(f, np.array([self.env.count_deaths]))\n",
        "        with open(\"mount/My Drive/Colab Notebooks/snake/episode\", 'wb') as f:\n",
        "            np.save(f, np.array([self.episode]))\n",
        "    \n",
        "    def api(self):\n",
        "        return (self.env.board, self.env.state, self.epsilon, self.mean_reward, \\\n",
        "                self.env.steps, self.env.count_deaths, \\\n",
        "                self.env.eaten_apples, self.episode, self.env.game_info)\n",
        "    \n",
        "    def calc_loss(self, batch, device=\"cpu\"):\n",
        "        # unpack batch\n",
        "        states, actions, rewards, dones, next_states = batch\n",
        "\n",
        "        # convert everything from batch to torch tensors and move it to device\n",
        "        states_v = torch.tensor(states).to(device, dtype=torch.float32)\n",
        "        next_states_v = torch.tensor(next_states).to(device, dtype=torch.float32)\n",
        "        actions_v = torch.tensor(actions).to(device, dtype=torch.int64)\n",
        "        rewards_v = torch.tensor(rewards).to(device, dtype=torch.float32)\n",
        "        done_mask = torch.ByteTensor(dones).to(device)\n",
        "        done_mask = done_mask.to(torch.bool)\n",
        "\n",
        "        # get output from NNs which is used for calculating state action value with discount\n",
        "        state_action_values = self.net.forward(states_v)\n",
        "        state_action_values = state_action_values.gather(1, actions_v.unsqueeze(-1)).squeeze(-1)\n",
        "        next_state_values = self.target_net.forward(next_states_v).max(1)[0]\n",
        "        next_state_values[done_mask] = 0.0\n",
        "        next_state_values = next_state_values.detach()\n",
        "\n",
        "        expected_state_action_values = next_state_values * GAMMA + rewards_v\n",
        "        # Calculate NN loss\n",
        "        return self.net.loss_f(state_action_values, expected_state_action_values)\n",
        "    \n",
        "    def simulate(self, print_info=True):\n",
        "        # Training AI\n",
        "        self.index += 1\n",
        "        self.epsilon = max(EPSILON_FINAL, EPSILON_START - self.index / EPSILON_DECAY_LAST_FRAME)\n",
        "\n",
        "        self.epsilon = 0.02\n",
        "\n",
        "        reward = self.play_step(self.net, self.env, self.epsilon, device=self.device)\n",
        "\n",
        "        if reward is not None:\n",
        "\n",
        "            self.total_rewards.append(reward)\n",
        "            self.mean_reward = np.mean(self.total_rewards[-100:])\n",
        "            \n",
        "            if self.best_mean_reward is None or self.best_mean_reward < self.mean_reward:\n",
        "                self.save()\n",
        "\n",
        "                if self.best_mean_reward is not None:\n",
        "                    self.agent_info = {\"Generation\": self.env.count_deaths, \\\n",
        "                                       \"Mean reward\": self.mean_reward, \\\n",
        "                                       \"Epsilon\": self.epsilon, \"Episode\": self.episode}\n",
        "                    if print_info: print(self.agent_info)\n",
        "\n",
        "                self.best_mean_reward = self.mean_reward\n",
        "\n",
        "            if self.env.game_info[\"won game\"]:\n",
        "                print(\"Solved in %d frames!\" % self.index)\n",
        "                self.save()\n",
        "                return\n",
        "\n",
        "            \"\"\"\n",
        "            if self.best_mean_reward > self.mean_reward and self.epsilon < 0.1:\n",
        "                self.episode += 1\n",
        "                self.load(load_index=False, load_episode=False)\n",
        "            \"\"\"\n",
        "\n",
        "        if len(self.buffer) < REPLAY_START_SIZE:\n",
        "            return\n",
        "        \n",
        "        # After certain amount time target net become first net\n",
        "        if self.index % SYNC_TARGET_LOOPS == 0:\n",
        "            self.target_net.load_state_dict(self.net.state_dict())\n",
        "          \n",
        "        # Calculate loss of NN and train it\n",
        "        self.net.optimizer.zero_grad()\n",
        "        batch = self.buffer.sample(BATCH_SIZE)\n",
        "        loss_t = self.calc_loss(batch, device=self.device)\n",
        "        loss_t.backward()\n",
        "        self.net.optimizer.step()"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bd6ThnYzUXk5"
      },
      "source": [
        "\n",
        "\n",
        "**Train**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmneYfS8UX0Q"
      },
      "source": [
        "    env = Environment(row)\n",
        "    buffer = ExperienceBuffer(REPLAY_SIZE)\n",
        "    net = Neural_Network()\n",
        "    dqn_agent = DQN(env, buffer, net, load=True)\n",
        "\n",
        "    flag = True\n",
        "    count = 0\n",
        "\n",
        "    while flag:      \n",
        "      all_boards = []\n",
        "      dqn_agent.simulate()\n",
        "      board, state, epsilon, mean_reward, steps, generation, score, episode, game_info = dqn_agent.api()\n",
        "\n",
        "      if game_info[\"won game\"]:\n",
        "          flag = False\n",
        "          print(\"finished\")\n",
        "      \n",
        "      if count % 50000 == 0:\n",
        "        print(\"Mean reward\", mean_reward, \"Generation\", generation, \"Epsilon\", epsilon, \"Mean Reward\", mean_reward)\n",
        "      count += 1"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}