{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SnakeQ_training.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"MB0lKXKXTJF1"},"source":["**AUTHORIZATIONS**"]},{"cell_type":"code","metadata":{"id":"urMGZWc_S6Uh"},"source":["from google.colab import drive\n","drive.mount('./mount')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jXulFYZFTLGm"},"source":["**IMPORT LIBRARIES**"]},{"cell_type":"code","metadata":{"id":"kKVNYPo7TPPb","executionInfo":{"status":"ok","timestamp":1602176586095,"user_tz":-120,"elapsed":983,"user":{"displayName":"Tomáš Novotný","photoUrl":"","userId":"10641169689505851937"}}},"source":["import collections\n","import numpy as np\n","\n","import torch\n","import torch.nn as nn\n","from torch import optim"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nW6NLtK0TfAg"},"source":["**Parameters**"]},{"cell_type":"code","metadata":{"id":"32M_uGnOTfJw","executionInfo":{"status":"ok","timestamp":1602176586719,"user_tz":-120,"elapsed":797,"user":{"displayName":"Tomáš Novotný","photoUrl":"","userId":"10641169689505851937"}}},"source":["# Env\n","ROW = 10\n","\n","# Q-learning\n","GAMMA = 0.99  # discount\n","BATCH_SIZE = 3\n","REPLAY_SIZE = 10000\n","LEARNING_RATE = 1e-4\n","SYNC_TARGET_LOOPS = 1000\n","REPLAY_START_SIZE = 10000\n","\n","# + epsilon -> chance for random action\n","EPSILON_DECAY_LAST_FRAME = 10**5\n","EPSILON_START = 1.0\n","EPSILON_FINAL = 0.008\n","\n","# Neural network\n","INPUT_SIZE = 28\n","N_ACTIONS = 4"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aakFnMylTpdb"},"source":["**Environment (Snake game)**"]},{"cell_type":"code","metadata":{"id":"LzdmGDMqTpkf","executionInfo":{"status":"ok","timestamp":1602176590848,"user_tz":-120,"elapsed":1847,"user":{"displayName":"Tomáš Novotný","photoUrl":"","userId":"10641169689505851937"}}},"source":["class SnakeSensors:\n","    def __init__(self, row, moves):\n","        self.row = row\n","        self.dis = self.row-1\n","        self.moves = moves\n","\n","        self.next_to_head_dir = [(1, 0), (0, 1), (-1, 0), (0, -1)]\n","    \n","    def update_sensor_board(self, board, snake):\n","        self.board = board\n","        self.head_y, self.head_x = snake\n","\n","        # distance to boundaries in 4 dimensions\n","        self.dis_y_down = self.dis - self.head_y\n","        self.dis_y_up = self.dis - self.dis_y_down\n","        self.dis_x_right = self.dis - self.head_x\n","        self.dis_x_left = self.dis - self.dis_x_right\n","\n","    def check_up(self, target):\n","        if self.head_y == 0: return 0\n","        for i in range(self.head_y-1, -1, -1):\n","            if self.board[i, self.head_x] == target:\n","                return 1\n","        return 0\n","    \n","    def check_down(self, target):\n","        if self.head_y == self.dis: return 0\n","        for i in range(self.head_y+1, self.row):\n","            if self.board[i, self.head_x] == target:\n","                return 1\n","        return 0\n","    \n","    def check_right(self, target):\n","        if self.head_x == self.dis: return 0\n","        for i in range(self.head_x+1, self.row):\n","            if self.board[self.head_y, i] == target:\n","                return 1\n","        return 0\n","    \n","    def check_left(self, target):\n","        if self.head_x == 0: return 0\n","        for i in range(self.head_x-1, -1, -1):\n","            if self.board[self.head_y, i] == target:\n","                return 1\n","        return 0\n","    \n","    def check_right_up(self, target):\n","        # choose shorter distance\n","        distance = self.dis_y_up if self.dis_y_up < self.dis_x_right else self.dis_x_right\n","        if distance == 0: return 0\n","        for n in range(1, distance+1):\n","            if self.board[self.head_y-n, self.head_x+n] == target:\n","                return 1\n","        return 0\n","    \n","    def check_right_down(self, target):\n","        distance = self.dis_y_down if self.dis_y_down < self.dis_x_right else self.dis_x_right\n","        if distance == self.row: return 0\n","        for n in range(1, distance+1):\n","            if self.board[self.head_y+n, self.head_x+n] == target: \n","                return 1\n","        return 0\n","    \n","    def check_left_up(self, target):\n","        distance = self.dis_y_up if self.dis_y_up < self.dis_x_left else self.dis_x_left\n","        if distance == 0: return 0\n","        for n in range(1, distance+1):\n","            if self.board[self.head_y-n, self.head_x-n] == target: \n","                return 1\n","        return 0\n","\n","    def check_left_down(self, target):\n","        distance = self.dis_y_down if self.dis_y_down < self.dis_x_left else self.dis_x_left\n","        if distance == self.row: return 0\n","        for n in range(1, distance+1):\n","            if self.board[self.head_y+n, self.head_x-n] == target: \n","                return 1\n","        return 0\n","    \n","    def all_eight_directions(self, target):\n","        to_up = self.check_up(target)                       # up\n","        to_right_up = self.check_right_up(target)           # up right\n","        to_right = self.check_right(target)                 # right\n","        to_right_down = self.check_right_down(target)       # right down\n","        to_down = self.check_down(target)                   # down\n","        to_left_down = self.check_left_down(target)         # down left\n","        to_left = self.check_left(target)                   # left\n","        to_left_up = self.check_left_up(target)             # left up\n","        return np.array([to_up, to_right_up, to_right, to_right_down, to_down, to_left_down, to_left, to_left_up])\n","\n","    def next_to_head(self, target):\n","        array = np.array([])\n","        for y, x in self.next_to_head_dir:\n","          if self.head_y+y > -1 and self.head_y+y < self.dis \\\n","            and self.head_x+x > -1 and self.head_x+x < self.dis:\n","            if self.board[self.head_y+y, self.head_x+x] == target:\n","              array = np.append(array, np.array([1]))\n","            else:   array = np.append(array, np.array([0]))\n","          else:     array = np.append(array, np.array([0]))\n","        return array\n","    \n","    def distance_to_walls(self):\n","        return np.round(np.array([self.dis_y_up, self.dis_x_right, self.dis_y_down, self.dis_x_left]) / self.dis, 1)\n","\n","    def get_head_direction(self, head_dir):\n","        if type(head_dir) == type(None):\n","          return np.array([0, 0, 0, 0])\n","        else:\n","          if np.array_equal(head_dir, self.moves[\"up\"]): return np.array([1, 0, 0, 0])        # up\n","          elif np.array_equal(head_dir, self.moves[\"right\"]): return np.array([0, 1, 0, 0])   # right\n","          elif np.array_equal(head_dir, self.moves[\"down\"]): return np.array([0, 0, 1, 0])    # down\n","          elif np.array_equal(head_dir, self.moves[\"left\"]): return np.array([0, 0, 0, 1])    # left\n","\n","    def get_tail_direction(self, snake):\n","        if len(snake)> 1:\n","            tail_dir = tuple((np.array([snake[1, 0], snake[1, 1]] \\\n","                 - np.array([snake[0, 0], snake[0, 1]]))).reshape(1, -1)[0])\n","            if np.array_equal(tail_dir, self.moves[\"up\"]): return np.array([1, 0, 0, 0])      # up\n","            elif np.array_equal(tail_dir, self.moves[\"right\"]): return np.array([0, 1, 0, 0]) # right\n","            elif np.array_equal(tail_dir, self.moves[\"down\"]): return np.array([0, 0, 1, 0])  # down\n","            elif np.array_equal(tail_dir, self.moves[\"left\"]): return np.array([0, 0, 0, 1])  # left\n","        else:\n","            return np.array([0, 0, 0, 0])\n","\n","### ENV\n","class Environment():\n","############################### Initalize parameters ###############################\n","    def __init__(self, n_row):\n","        # row * row = board\n","        self.row = n_row\n","        # map of board\n","        self.blocks = {\"empty\": 0, \"snake\": 1, \"apple\": 2}\n","        # (y, x)\n","        self.moves_dir = {\"up\": np.array([-1, 0]), \"right\": np.array([0, 1]), \\\n","                      \"down\": np.array([1, 0]), \"left\": np.array([0, -1])}\n","        # List of all rewards\n","        self.reward_dict = {\"hit self\": -100, \"hit boundary\": -100, \"eat apple\": 10, \\\n","                            \"step\": -1, \"a lot of steps\": -100, \"win game\": 1000}\n","        # Number of possible actions\n","        self.action_space = 4\n","        # Set up sensors for getting state\n","        self.Sensors = SnakeSensors(self.row, self.moves_dir)\n","        # Prepare game\n","        self.reset()\n","\n","    def second_init(self):\n","        self.done = False           # If game is over (death)\n","        self.direction = None       # Direction of head (for computing state)\n","        self.steps = 0              # count of steps until it reache apple\n","        self.eaten_apples = 0       # count of eaten apples\n","        self.info = \"Unfinished\"    # If player win the game\n","\n","############################### GENERATING ###############################\n","    def generate_grid(self):\n","        # generate board (grid) of zeros (always square)\n","        self.board = np.zeros((self.row, self.row))\n","    \n","    def generate_snake(self):\n","        # Randomly choose spot to generate snake\n","        indices = np.random.randint(0, high=self.row, size=2)\n","        y, x = indices[0], indices[1]\n","\n","        self.board[y, x] = self.blocks[\"snake\"]\n","        self.snake_body = np.array([[y, x]])\n","        self.beginning_lenght = 1\n","\n","    def generate_apple(self):\n","        # Randomly generate apple (if there isn't already body of snake)\n","        while True:\n","            indices = np.random.randint(0, high=self.row, size=2)\n","            y, x = indices[0], indices[1]\n","\n","            if self.board[y, x] == self.blocks[\"empty\"]:\n","                self.board[y, x] = self.blocks[\"apple\"]\n","                self.apple_pos = np.array([y, x])\n","                break\n","\n","############################### CHECK LOGIC ###############################\n","    def check_n_steps(self):\n","        # If count of steps is bigger than treshold; game over\n","        if self.steps > (self.row**2/2):\n","            self.done = True\n","            self.reward = self.reward_dict[\"a lot of steps\"]\n","\n","    def check_hit_self(self):\n","        # Check if set of body isn't long as it had eaten apples\n","        self.body = set([(i[0], i[1]) for i in self.snake_body.tolist()])\n","        self.len_body = len(self.body)\n","        if len(self.body) != self.eaten_apples+self.beginning_lenght:\n","            self.done = True\n","            self.reward = self.reward_dict[\"hit self\"]\n","\n","    def check_boundaries(self, new_head):\n","        # Check if (y, x) go beyond boundary\n","        y, x = new_head\n","        if y < 0 or x < 0 or y > self.row-1 or x > self.row-1:\n","            self.done = True\n","            self.reward = self.reward_dict[\"hit boundary\"]\n","    \n","    def check_end_of_game(self):\n","        # If whole board is filled with snake; player won\n","        if np.all(self.board.all(self.blocks[\"snake\"])):\n","            self.done = True\n","            self.reward = self.reward_dict[\"win game\"]\n","            self.info = \"Finished\"\n","    \n","    def check_eaten_apple(self, head):\n","        # If head is on position of apple; restart steps and update other components...\n","        if np.array_equal(head, self.apple_pos):\n","            self.steps = 0\n","            self.eaten_apples += 1\n","            self.generate_apple()\n","            self.reward = self.reward_dict[\"eat apple\"] + len(self.snake_body)**2\n","            return True\n","        return False\n","\n","    def snake_algorithm(self, new_head):\n","        # Set new head of snake before current head in corresponding direction\n","        self.snake_body = np.vstack((self.snake_body, new_head))\n","\n","        # if eaten apple == False; tail is deleted\n","        if not self.check_eaten_apple(self.snake_body[-1]):\n","            self.snake_body = np.delete(self.snake_body, 0, 0)\n","    \n","    def move(self, action):\n","        # handling whole logic\n","        if action == 0:     direction = self.moves_dir[\"up\"]\n","        elif action == 1:   direction = self.moves_dir[\"right\"]\n","        elif action == 2:   direction = self.moves_dir[\"down\"]\n","        elif action == 3:   direction = self.moves_dir[\"left\"]\n","        self.direction = direction\n","        head_pos = self.snake_body[-1]\n","        new_head_pos = (head_pos[0]+direction[0], head_pos[1]+direction[1])\n","\n","        self.check_n_steps()\n","        self.check_hit_self()\n","        self.check_boundaries(new_head_pos)\n","        if not self.done:\n","            self.snake_algorithm(new_head_pos)\n","\n","    def compute_state(self):\n","        # Compute state of snake sensors from SnakeSensors; get passed to Agent\n","        self.Sensors.update_sensor_board(self.board, self.snake_body[-1])\n","        #next_to_head = self.Sensors.next_to_head(self.blocks[\"empty\"])\n","        distance = self.Sensors.distance_to_walls()\n","        see_apple = self.Sensors.all_eight_directions(self.blocks[\"apple\"])\n","        see_self = self.Sensors.all_eight_directions(self.blocks[\"snake\"])\n","        head_dir = self.Sensors.get_head_direction(self.direction)\n","        tail_dir = self.Sensors.get_tail_direction(self.snake_body)\n","        self.state = np.concatenate((distance, see_apple, see_self, head_dir, tail_dir), axis=0)\n","\n","############################### PERFORM FUNCTIONS FOR ENV ###############################\n","    def sample_action(self):\n","        # return random action\n","        return np.random.choice(np.array([0, 1, 2, 3]), size=1)[0]\n","    \n","    def refresh_board(self):\n","        # refresh board; write on board snake and apple\n","        self.generate_grid()\n","        for body in self.snake_body:\n","            self.board[body[0], body[1]] = self.blocks[\"snake\"]\n","        self.board[self.apple_pos[0], self.apple_pos[1]] = self.blocks[\"apple\"]\n","\n","    def reset(self):\n","        # Reset/set up game parameters\n","        self.second_init()\n","\n","        # Generate and refresh board\n","        self.generate_grid()\n","        self.generate_snake()\n","        self.generate_apple()\n","        self.compute_state()\n","\n","        return self.state\n","\n","    def step(self, action):\n","        # Perform action, whole back up logic and return results of action\n","        self.reward = self.reward_dict[\"step\"]\n","        self.steps += 1\n","        if self.done:\n","            self.reset()\n","\n","        self.move(action)\n","        self.refresh_board()\n","        self.compute_state()\n","\n","        return self.state, self.reward, self.done, self.info"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q_TkZ9Vb0jas"},"source":["**AGENT**"]},{"cell_type":"code","metadata":{"id":"wfcd3YMW0ncW","executionInfo":{"status":"ok","timestamp":1602176590852,"user_tz":-120,"elapsed":495,"user":{"displayName":"Tomáš Novotný","photoUrl":"","userId":"10641169689505851937"}}},"source":["Experience = collections.namedtuple('Experience', field_names=['state', 'action', 'reward', 'done', 'new_state'])\n","\n","class ExperienceBuffer:\n","    def __init__(self, capacity):\n","        self.buffer = collections.deque(maxlen=capacity)\n","\n","    def __len__(self):\n","        return len(self.buffer)\n","\n","    def append(self, experience):\n","        self.buffer.append(experience)\n","\n","    def sample(self, batch_size):\n","        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n","        states, actions, rewards, dones, next_states = zip(*[self.buffer[idx] for idx in indices])\n","        return np.array(states), np.array(actions), np.array(rewards, dtype=np.float32), \\\n","               np.array(dones, dtype=np.uint8), np.array(next_states)\n","\n","class Agent:\n","    def __init__(self, env, exp_buffer):\n","        self.env = env\n","        self.reset_env = env\n","        self.exp_buffer = exp_buffer\n","        self._reset()\n","        self.generation_count = 0\n","\n","    def _reset(self):\n","        #self.state = self.env.reset()\n","        self.state = self.reset_env.reset()\n","        self.total_reward = 0.0\n","    \n","    @torch.no_grad()\n","    def play_step(self, net, epsilon=0.0, device=\"cpu\"):\n","        done_reward = None\n","\n","        if np.random.random() < epsilon:\n","            action = self.env.sample_action()\n","        else:\n","            state_a = np.array([self.state], copy=False).astype(\"float32\")\n","            state_v = torch.from_numpy(state_a).to(device, dtype=torch.float32)\n","            q_vals_v = net(state_v)\n","            _, action_v = torch.max(q_vals_v, dim=1)\n","            action = int(action_v.item())\n","\n","        # do step in the environment\n","        new_state, reward, is_done, _ = self.env.step(action)\n","        self.total_reward += reward\n","\n","        exp = Experience(self.state, action, reward, is_done, new_state)\n","        self.exp_buffer.append(exp)\n","        self.state = new_state\n","\n","        if is_done:\n","            done_reward = self.total_reward\n","            self._reset()\n","            self.generation_count += 1\n","        return done_reward"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LxCuAG-1UADt"},"source":["**DQN AI**"]},{"cell_type":"code","metadata":{"id":"pDYWb6UDUAOA","executionInfo":{"status":"ok","timestamp":1602176593091,"user_tz":-120,"elapsed":1444,"user":{"displayName":"Tomáš Novotný","photoUrl":"","userId":"10641169689505851937"}}},"source":["class Neural_Network(nn.Module):\n","    def __init__(self, lr=LEARNING_RATE):\n","        super(Neural_Network, self).__init__()\n","        \"\"\"\n","        Input to NN:\n","            [distance to wall, see apple, see it self, head direction, tail direction] -> 28 elements\n","        output of NN:\n","            [0: up    1: right    2: down    3: left] -> 4 elements\n","        \"\"\"\n","        \n","        self.model = nn.Sequential(\n","            nn.Linear(INPUT_SIZE, 20),\n","            nn.ReLU(),\n","            nn.Linear(20, 12),\n","            nn.ReLU(),\n","            nn.Linear(12, N_ACTIONS)\n","        )\n","\n","        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n","    \n","    def forward(self, input_tensor):\n","        return self.model(input_tensor)\n","\n","class SaveAndLoad:\n","    def load_models(self, net, target_net, agent, device):\n","        net.load_state_dict(torch.load(\"mount/My Drive/Colab Notebooks/snake/net.dat\", map_location=torch.device(device)))\n","        target_net.load_state_dict(net.state_dict())\n","        with open(\"mount/My Drive/Colab Notebooks/snake/index\", 'rb') as f:\n","            index = np.load(f)[0]\n","        with open(\"mount/My Drive/Colab Notebooks/snake/total_rewards\", 'rb') as f:\n","            total_rewards = np.load(f).tolist()\n","        with open(\"mount/My Drive/Colab Notebooks/snake/count_deaths\", 'rb') as f:\n","            agent.generation_count = np.load(f)[0]\n","        return net, target_net, index, total_rewards\n","    \n","    def save_models(self, net, agent, index, total_rewards):\n","        torch.save(net.state_dict(), \"mount/My Drive/Colab Notebooks/snake/net.dat\")\n","        with open(\"mount/My Drive/Colab Notebooks/snake/index\", 'wb') as f:\n","            np.save(f, np.array([index]))\n","        with open(\"mount/My Drive/Colab Notebooks/snake/total_rewards\", 'wb') as f:\n","            np.save(f, np.array(total_rewards))\n","        with open(\"mount/My Drive/Colab Notebooks/snake/count_deaths\", 'wb') as f:\n","            np.save(f, np.array([agent.generation_count]))\n","\n","class DQN(SaveAndLoad):\n","    def __init__(self, net, buffer, agent, load=False):\n","        self.device = self.select_device()\n","        self.net = net.to(self.device)\n","        self.target_net = net.to(self.device)\n","        self.buffer = buffer\n","        self.agent = agent\n","\n","        self.epsilon = EPSILON_START\n","        self.lr = LEARNING_RATE\n","\n","        self.second_init(load)\n","    \n","    def second_init(self, load):\n","        # parameters\n","        self.best_mean_reward = None\n","        self.mean_reward = None\n","        self.finished = False\n","        # loading sequence\n","        if load: \n","            self.net, self.target_net, \\\n","                self.index, self.total_rewards = \\\n","                    self.load_models(self.net, self.target_net, self.agent, self.device)\n","        else:\n","            self.index = 0\n","            self.total_rewards = []\n","\n","    def select_device(self):\n","        if torch.cuda.is_available():\n","            torch.set_default_tensor_type(torch.cuda.FloatTensor)\n","            print(\"using cuda:\", torch.cuda.get_device_name(0))\n","        return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    \n","    def save(self):\n","        self.save_models(self.net, self.agent, self.index, self.total_rewards)\n","    \n","    def api(self):    \n","        return (self.agent.env.board, self.agent.env.state, self.agent.env.reward, \\\n","                self.epsilon, self.mean_reward, \\\n","                self.agent.env.steps, self.agent.generation_count, \\\n","                self.agent.env.eaten_apples)\n","    \n","    def light_api(self):\n","        return (self.epsilon, self.mean_reward, \\\n","                self.agent.env.steps, self.agent.generation_count, \\\n","                self.agent.env.eaten_apples)\n","    \n","    def super_light_api(self):\n","        return (self.agent.env.info)\n","    \n","    def calc_loss(self, batch, device=\"cpu\"):\n","        # unpack batch\n","        states, actions, rewards, dones, next_states = batch\n","\n","        # convert everything from batch to torch tensors and move it to device\n","        states_v = torch.tensor(states).to(device, dtype=torch.float32)\n","        next_states_v = torch.tensor(next_states).to(device, dtype=torch.float32)\n","        actions_v = torch.tensor(actions).to(device, dtype=torch.int64)\n","        rewards_v = torch.tensor(rewards).to(device, dtype=torch.float32)\n","        done_mask = torch.ByteTensor(dones).to(device)\n","        done_mask = done_mask.to(torch.bool)\n","\n","        # get output from NNs which is used for calculating state action value with discount\n","        state_action_values = self.net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)\n","        next_state_values = self.target_net(next_states_v).max(1)[0]\n","        next_state_values[done_mask] = 0.0\n","        next_state_values = next_state_values.detach()\n","\n","        expected_state_action_values = next_state_values * GAMMA + rewards_v\n","        # Calculate NN loss\n","        return nn.MSELoss()(state_action_values, expected_state_action_values)\n","    \n","    def simulate(self):\n","        # Training AI\n","        self.index += 1\n","        self.epsilon = max(EPSILON_FINAL, EPSILON_START - self.index / EPSILON_DECAY_LAST_FRAME)\n","\n","        reward = self.agent.play_step(self.net, self.epsilon, device=self.device)\n","\n","        if reward is not None:\n","            self.total_rewards.append(reward)\n","            self.mean_reward = np.mean(self.total_rewards[-100:])\n","            \n","            if self.best_mean_reward is None or self.best_mean_reward < self.mean_reward:\n","                self.save()\n","                self.total_rewards = self.total_rewards[-100:]\n","\n","                if self.best_mean_reward is not None:\n","                    self.agent_info = {\"Generation\": self.agent.generation_count, \"Mean reward\": self.mean_reward, \"Epsilon\": self.epsilon}\n","                    print(self.agent_info)\n","                self.best_mean_reward = self.mean_reward\n","\n","            if self.agent.env.info == \"Finished\":\n","                print(\"Solved in %d frames!\" % self.index)\n","                self.save()\n","                self.finished = True\n","                return\n","            \n","        if len(self.buffer) < REPLAY_START_SIZE:\n","            return\n","        \n","        # After certain amount time target net become first net\n","        if self.index % SYNC_TARGET_LOOPS == 0:\n","            self.target_net.load_state_dict(self.net.state_dict())\n","\n","        # Calculate loss of NN and train it\n","        self.net.optimizer.zero_grad()\n","        batch = self.buffer.sample(BATCH_SIZE)\n","        loss_t = self.calc_loss(batch, device=self.device)\n","        loss_t.backward()\n","        self.net.optimizer.step()"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Bd6ThnYzUXk5"},"source":["\n","\n","**Train**"]},{"cell_type":"code","metadata":{"id":"MmneYfS8UX0Q","outputId":"1279b50b-d2d0-4d05-e1ec-3e48b7483540","colab":{"base_uri":"https://localhost:8080/","height":972}},"source":["net = Neural_Network()\n","env = Environment(ROW)\n","buffer = ExperienceBuffer(REPLAY_SIZE)\n","agent = Agent(env, buffer)\n","dqn = DQN(net, buffer, agent, load=False)\n","\n","flag = True\n","count = 0\n","\n","while True:\n","    dqn.simulate()\n","\n","    if dqn.super_light_api() == \"Finished\":\n","        break\n","\n","    if count % 100000 == 0:\n","        epsilon, mean_reward, steps, generation, score = dqn.light_api()\n","        print(\"Generation\", generation, \"Mean reward\", mean_reward, \"Epsilon\", epsilon, \"Mean Reward\", mean_reward)\n","        dqn.save()\n","        count = 0\n","    count += 1"],"execution_count":null,"outputs":[{"output_type":"stream","text":["using cuda: Tesla K80\n","Generation 0 Mean reward None Epsilon 0.99999 Mean Reward None\n","{'Generation': 2, 'Mean reward': -127.5, 'Epsilon': 0.99943}\n","{'Generation': 3, 'Mean reward': -122.66666666666667, 'Epsilon': 0.99929}\n","{'Generation': 4, 'Mean reward': -118.25, 'Epsilon': 0.99923}\n","{'Generation': 5, 'Mean reward': -115.6, 'Epsilon': 0.99917}\n","{'Generation': 6, 'Mean reward': -111.66666666666667, 'Epsilon': 0.99909}\n","{'Generation': 7, 'Mean reward': -110.42857142857143, 'Epsilon': 0.99905}\n","{'Generation': 8, 'Mean reward': -109.25, 'Epsilon': 0.99903}\n","{'Generation': 1749, 'Mean reward': -108.74, 'Epsilon': 0.72139}\n","{'Generation': 2134, 'Mean reward': -108.72, 'Epsilon': 0.66933}\n","{'Generation': 2145, 'Mean reward': -108.67, 'Epsilon': 0.6679200000000001}\n","{'Generation': 2146, 'Mean reward': -108.66, 'Epsilon': 0.66791}\n","{'Generation': 2147, 'Mean reward': -108.61, 'Epsilon': 0.66774}\n","{'Generation': 2148, 'Mean reward': -108.35, 'Epsilon': 0.66771}\n","{'Generation': 2149, 'Mean reward': -108.2, 'Epsilon': 0.66769}\n","{'Generation': 2150, 'Mean reward': -108.08, 'Epsilon': 0.66745}\n","{'Generation': 3266, 'Mean reward': -108.05, 'Epsilon': 0.5237}\n","{'Generation': 3753, 'Mean reward': -107.84, 'Epsilon': 0.45740000000000003}\n","{'Generation': 3754, 'Mean reward': -107.54, 'Epsilon': 0.45738999999999996}\n","{'Generation': 3755, 'Mean reward': -107.23, 'Epsilon': 0.45738}\n","{'Generation': 3756, 'Mean reward': -106.99, 'Epsilon': 0.45725000000000005}\n","{'Generation': 3758, 'Mean reward': -106.97, 'Epsilon': 0.45723}\n","{'Generation': 3761, 'Mean reward': -106.89, 'Epsilon': 0.45692}\n","{'Generation': 3763, 'Mean reward': -106.84, 'Epsilon': 0.45668}\n","{'Generation': 3768, 'Mean reward': -106.33, 'Epsilon': 0.45635000000000003}\n","{'Generation': 3771, 'Mean reward': -106.08, 'Epsilon': 0.45604999999999996}\n","{'Generation': 3775, 'Mean reward': -106.06, 'Epsilon': 0.45576000000000005}\n","{'Generation': 3776, 'Mean reward': -106.04, 'Epsilon': 0.45565999999999995}\n","{'Generation': 4746, 'Mean reward': -105.61, 'Epsilon': 0.33914}\n","{'Generation': 4747, 'Mean reward': -105.45, 'Epsilon': 0.33909}\n","{'Generation': 4748, 'Mean reward': -105.35, 'Epsilon': 0.33901000000000003}\n","{'Generation': 5953, 'Mean reward': -105.31, 'Epsilon': 0.2086}\n","{'Generation': 5955, 'Mean reward': -105.23, 'Epsilon': 0.20830000000000004}\n","{'Generation': 5957, 'Mean reward': -105.21, 'Epsilon': 0.20811999999999997}\n","{'Generation': 5964, 'Mean reward': -105.18, 'Epsilon': 0.20765999999999996}\n","{'Generation': 5965, 'Mean reward': -105.06, 'Epsilon': 0.20764000000000005}\n","{'Generation': 5976, 'Mean reward': -104.76, 'Epsilon': 0.20675}\n","Generation 7738 Mean reward -114.56 Epsilon 0.008 Mean Reward -114.56\n","{'Generation': 8976, 'Mean reward': -104.29, 'Epsilon': 0.008}\n","{'Generation': 8977, 'Mean reward': -103.83, 'Epsilon': 0.008}\n","{'Generation': 8979, 'Mean reward': -103.81, 'Epsilon': 0.008}\n","{'Generation': 11928, 'Mean reward': -103.59, 'Epsilon': 0.008}\n","{'Generation': 11931, 'Mean reward': -103.37, 'Epsilon': 0.008}\n","{'Generation': 11933, 'Mean reward': -103.36, 'Epsilon': 0.008}\n","Generation 15199 Mean reward -119.25 Epsilon 0.008 Mean Reward -119.25\n","Generation 19754 Mean reward -108.9 Epsilon 0.008 Mean Reward -108.9\n","{'Generation': 20888, 'Mean reward': -103.04, 'Epsilon': 0.008}\n","{'Generation': 20889, 'Mean reward': -102.63, 'Epsilon': 0.008}\n","{'Generation': 20890, 'Mean reward': -102.13, 'Epsilon': 0.008}\n","{'Generation': 20891, 'Mean reward': -101.61, 'Epsilon': 0.008}\n","Generation 23250 Mean reward -133.71 Epsilon 0.008 Mean Reward -133.71\n"],"name":"stdout"}]}]}